{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models with different latent sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from lfads import LFADS_Net, LFADS_SingleSession_Net\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "from scheduler import LFADS_Scheduler\n",
    "from plotter import Plotter\n",
    "from trainer import RunManager\n",
    "\n",
    "import yaml\n",
    "\n",
    "from synthetic_data import LorenzSystem, EmbeddedLowDNetwork\n",
    "from objective import SVLAE_Loss, LFADS_Loss, LogLikelihoodPoisson, LogLikelihoodPoissonSimplePlusL1, LogLikelihoodPoissonSimple, LogLikelihoodGaussian\n",
    "\n",
    "from utils import load_parameters\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14007/2887542036.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_beh[\"PrevResponse\"] = valid_beh[\"Response\"].shift()\n"
     ]
    }
   ],
   "source": [
    "species = 'nhp'\n",
    "subject = 'SA'\n",
    "exp = 'WCST'\n",
    "session = 20180802  # this is the session for which there are spikes at the moment. \n",
    "\n",
    "NHP_WCST_DIR = 'nhp-lfp/wcst-preprocessed/'\n",
    "\n",
    "\n",
    "# grab behavioral data, spike data, trial numbers. \n",
    "fs = s3fs.S3FileSystem()\n",
    "behavior_file = os.path.join(NHP_WCST_DIR, \"rawdata\", \"sub-\" + str(subject), \"sess-\" + str(session), \"behavior\", \"sub-\" + str(subject) + \"_sess-\" + str(session) + \"_object_features.csv\")\n",
    "behavior_data = pd.read_csv(fs.open(behavior_file))\n",
    "valid_beh = behavior_data[behavior_data.Response.isin([\"Correct\", \"Incorrect\"])]   \n",
    "valid_beh[\"PrevResponse\"] = valid_beh[\"Response\"].shift()  \n",
    "valid_beh = valid_beh[valid_beh.TrialNumber >= 57]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_interval = 0\n",
    "post_interval = 350\n",
    "interval_size = 50\n",
    "frs = pd.read_pickle(fs.open(f\"l2l.pqz317.scratch/firing_rates_{pre_interval}_crossfixation_{post_interval}_{interval_size}_bins.pickle\"))\n",
    "frs = frs[frs.TrialNumber.isin(valid_beh.TrialNumber)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data in trials x time x neurons\n",
    "num_time_bins = len(frs[\"TimeBins\"].unique())\n",
    "num_units = len(frs[\"UnitID\"].unique())\n",
    "num_trials = len(frs[\"TrialNumber\"].unique())\n",
    "sorted = frs.sort_values(by=[\"UnitID\", \"TimeBins\", \"TrialNumber\"])\n",
    "# currently in neurons x time x trials\n",
    "spike_data = sorted[\"SpikeCounts\"].to_numpy().reshape((num_units, num_time_bins, num_trials))\n",
    "# want trials x time x neurons\n",
    "spike_data = np.transpose(spike_data, (2, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'; print(device)\n",
    "device = 'cpu'\n",
    "train_idxs, valid_idxs = train_test_split(\n",
    "    np.arange(spike_data.shape[1]), \n",
    "    train_size=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "train_spikes = spike_data[:, train_idxs, :]\n",
    "\n",
    "valid_spikes = spike_data[:, valid_idxs, :]\n",
    "\n",
    "train_data  = torch.Tensor(train_spikes).to(device)\n",
    "valid_data  = torch.Tensor(valid_spikes).to(device)\n",
    "train_ds    = torch.utils.data.TensorDataset(train_data)\n",
    "valid_ds    = torch.utils.data.TensorDataset(valid_data)\n",
    "train_dl    = torch.utils.data.DataLoader(train_ds, batch_size = 50, shuffle=True)\n",
    "valid_dl    = torch.utils.data.DataLoader(valid_ds, batch_size = valid_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_path = 'lfads_cross_fixation_hyperparams.yaml'\n",
    "hyperparams = load_parameters(hyperparameter_path)\n",
    "\n",
    "dt = 0.05\n",
    "loglikelihood = LogLikelihoodPoisson(dt=dt, device=device)\n",
    "objective = LFADS_Loss(\n",
    "    loglikelihood=loglikelihood,\n",
    "    loss_weight_dict={\n",
    "        \"kl\": hyperparams['objective'][\"kl\"],\n",
    "        \"l2\": hyperparams[\"objective\"][\"l2\"]},\n",
    "    l2_con_scale=hyperparams[\"objective\"][\"l2_con_scale\"],\n",
    "    l2_gen_scale=hyperparams[\"objective\"][\"l2_gen_scale\"]\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     1, Epoch time = 0.429 s, Loss (train, valid):  recon (112.676, 43.649), kl (1.556, 0.341), total (117.909, 48.103), l2 (3.677)\n",
      "Epoch     2, Epoch time = 0.426 s, Loss (train, valid):  recon (103.006, 43.617), kl (0.336, 0.402), total (107.049, 47.307), l2 (3.707)\n",
      "Epoch     3, Epoch time = 0.426 s, Loss (train, valid):  recon (102.630, 43.721), kl (0.410, 0.411), total (105.974, 46.717), l2 (2.933)\n",
      "Epoch     4, Epoch time = 0.451 s, Loss (train, valid):  recon (102.361, 44.188), kl (0.519, 0.309), total (105.185, 46.533), l2 (2.306)\n",
      "Epoch     5, Epoch time = 0.431 s, Loss (train, valid):  recon (102.115, 43.761), kl (0.519, 0.254), total (104.458, 45.633), l2 (1.823)\n",
      "Epoch     6, Epoch time = 0.427 s, Loss (train, valid):  recon (101.925, 43.688), kl (0.612, 0.296), total (103.991, 45.273), l2 (1.454)\n",
      "Epoch     7, Epoch time = 0.452 s, Loss (train, valid):  recon (101.776, 44.021), kl (0.624, 0.261), total (103.567, 45.325), l2 (1.167)\n",
      "Epoch     8, Epoch time = 0.417 s, Loss (train, valid):  recon (101.728, 43.914), kl (0.646, 0.273), total (103.322, 45.043), l2 (0.948)\n",
      "Epoch     9, Epoch time = 0.424 s, Loss (train, valid):  recon (101.491, 44.103), kl (0.683, 0.270), total (102.955, 45.079), l2 (0.781)\n",
      "Epoch    10, Epoch time = 0.431 s, Loss (train, valid):  recon (101.165, 43.825), kl (0.695, 0.236), total (102.509, 44.651), l2 (0.648)\n",
      "Epoch    11, Epoch time = 0.414 s, Loss (train, valid):  recon (100.988, 44.142), kl (0.749, 0.262), total (102.283, 44.903), l2 (0.546)\n",
      "Epoch    12, Epoch time = 0.422 s, Loss (train, valid):  recon (100.753, 43.948), kl (0.777, 0.262), total (101.993, 44.638), l2 (0.463)\n",
      "Epoch    13, Epoch time = 0.425 s, Loss (train, valid):  recon (100.613, 44.069), kl (0.811, 0.263), total (101.822, 44.702), l2 (0.399)\n",
      "Epoch    14, Epoch time = 0.424 s, Loss (train, valid):  recon (100.463, 44.086), kl (0.849, 0.251), total (101.661, 44.662), l2 (0.349)\n",
      "Epoch    15, Epoch time = 0.422 s, Loss (train, valid):  recon (100.563, 43.950), kl (0.827, 0.229), total (101.698, 44.475), l2 (0.309)\n",
      "Epoch    16, Epoch time = 0.423 s, Loss (train, valid):  recon (100.353, 44.391), kl (0.841, 0.277), total (101.475, 44.937), l2 (0.281)\n",
      "Epoch    17, Epoch time = 0.417 s, Loss (train, valid):  recon (100.323, 44.261), kl (0.824, 0.172), total (101.406, 44.683), l2 (0.259)\n",
      "Epoch    18, Epoch time = 0.420 s, Loss (train, valid):  recon (100.366, 43.760), kl (0.820, 0.237), total (101.425, 44.229), l2 (0.239)\n",
      "Epoch    19, Epoch time = 0.421 s, Loss (train, valid):  recon (100.250, 44.198), kl (0.867, 0.218), total (101.346, 44.642), l2 (0.229)\n",
      "Epoch    20, Epoch time = 0.434 s, Loss (train, valid):  recon (100.179, 44.071), kl (0.880, 0.204), total (101.280, 44.492), l2 (0.221)\n",
      "Epoch    21, Epoch time = 0.424 s, Loss (train, valid):  recon (100.069, 44.170), kl (0.847, 0.209), total (101.129, 44.590), l2 (0.214)\n",
      "Epoch    22, Epoch time = 0.421 s, Loss (train, valid):  recon (100.290, 43.937), kl (0.841, 0.231), total (101.342, 44.381), l2 (0.211)\n",
      "Epoch    23, Epoch time = 0.410 s, Loss (train, valid):  recon (100.106, 44.063), kl (0.857, 0.267), total (101.177, 44.542), l2 (0.214)\n",
      "Epoch    24, Epoch time = 0.445 s, Loss (train, valid):  recon (100.061, 44.174), kl (0.871, 0.219), total (101.141, 44.600), l2 (0.209)\n",
      "Epoch    25, Epoch time = 0.443 s, Loss (train, valid):  recon (100.061, 44.054), kl (0.825, 0.251), total (101.094, 44.511), l2 (0.207)\n",
      "Epoch    26, Epoch time = 0.429 s, Loss (train, valid):  recon (100.144, 44.128), kl (0.879, 0.222), total (101.230, 44.554), l2 (0.208)\n",
      "Epoch    27, Epoch time = 0.510 s, Loss (train, valid):  recon (99.967, 43.969), kl (0.883, 0.246), total (101.054, 44.415), l2 (0.204)\n",
      "Epoch    28, Epoch time = 0.417 s, Loss (train, valid):  recon (99.963, 44.570), kl (0.867, 0.206), total (101.026, 44.975), l2 (0.197)\n",
      "Epoch    29, Epoch time = 0.426 s, Loss (train, valid):  recon (99.923, 44.167), kl (0.874, 0.272), total (100.994, 44.629), l2 (0.197)\n",
      "Epoch    30, Epoch time = 0.424 s, Loss (train, valid):  recon (99.886, 44.380), kl (0.899, 0.190), total (100.975, 44.760), l2 (0.190)\n",
      "Epoch    31, Epoch time = 0.479 s, Loss (train, valid):  recon (99.920, 44.370), kl (0.871, 0.219), total (100.978, 44.777), l2 (0.187)\n",
      "Epoch    32, Epoch time = 0.421 s, Loss (train, valid):  recon (99.837, 44.167), kl (0.896, 0.224), total (100.918, 44.574), l2 (0.186)\n",
      "Epoch    33, Epoch time = 0.430 s, Loss (train, valid):  recon (99.841, 43.937), kl (0.900, 0.250), total (100.925, 44.368), l2 (0.184)\n",
      "Epoch    34, Epoch time = 0.418 s, Loss (train, valid):  recon (99.870, 44.167), kl (0.853, 0.210), total (100.902, 44.554), l2 (0.180)\n",
      "Epoch    35, Epoch time = 0.433 s, Loss (train, valid):  recon (99.970, 44.172), kl (0.880, 0.246), total (101.023, 44.590), l2 (0.173)\n",
      "Epoch    36, Epoch time = 0.433 s, Loss (train, valid):  recon (99.844, 44.369), kl (0.874, 0.198), total (100.894, 44.741), l2 (0.176)\n",
      "Epoch    37, Epoch time = 0.418 s, Loss (train, valid):  recon (99.843, 44.220), kl (0.896, 0.209), total (100.911, 44.599), l2 (0.172)\n",
      "Epoch    38, Epoch time = 0.403 s, Loss (train, valid):  recon (99.987, 44.089), kl (0.847, 0.218), total (101.003, 44.476), l2 (0.170)\n",
      "Epoch    39, Epoch time = 0.426 s, Loss (train, valid):  recon (99.906, 43.942), kl (0.884, 0.298), total (100.962, 44.411), l2 (0.171)\n",
      "Epoch    40, Epoch time = 0.421 s, Loss (train, valid):  recon (99.737, 44.266), kl (0.857, 0.224), total (100.763, 44.658), l2 (0.168)\n",
      "Epoch    41, Epoch time = 0.443 s, Loss (train, valid):  recon (99.756, 44.092), kl (0.869, 0.191), total (100.792, 44.450), l2 (0.168)\n",
      "Epoch    42, Epoch time = 0.445 s, Loss (train, valid):  recon (99.824, 44.168), kl (0.885, 0.182), total (100.874, 44.511), l2 (0.165)\n",
      "Epoch    43, Epoch time = 0.409 s, Loss (train, valid):  recon (100.009, 44.140), kl (0.860, 0.237), total (101.032, 44.539), l2 (0.164)\n",
      "Epoch    44, Epoch time = 0.419 s, Loss (train, valid):  recon (99.827, 44.268), kl (0.895, 0.242), total (100.882, 44.673), l2 (0.161)\n",
      "Epoch    45, Epoch time = 0.422 s, Loss (train, valid):  recon (99.822, 44.203), kl (0.883, 0.194), total (100.868, 44.560), l2 (0.163)\n",
      "Epoch    46, Epoch time = 0.442 s, Loss (train, valid):  recon (99.768, 44.468), kl (0.881, 0.176), total (100.814, 44.804), l2 (0.164)\n",
      "Epoch 00047: reducing learning rate of group 0 to 9.5000e-03.\n",
      "Epoch    47, Epoch time = 0.420 s, Loss (train, valid):  recon (99.725, 44.160), kl (0.879, 0.228), total (100.764, 44.548), l2 (0.160)\n",
      "Epoch    48, Epoch time = 0.422 s, Loss (train, valid):  recon (99.769, 44.212), kl (0.884, 0.197), total (100.811, 44.569), l2 (0.158)\n",
      "Epoch    49, Epoch time = 0.428 s, Loss (train, valid):  recon (99.651, 43.816), kl (0.882, 0.242), total (100.690, 44.213), l2 (0.157)\n",
      "Epoch    50, Epoch time = 0.414 s, Loss (train, valid):  recon (99.741, 44.186), kl (0.881, 0.169), total (100.777, 44.511), l2 (0.155)\n",
      "Epoch    51, Epoch time = 0.435 s, Loss (train, valid):  recon (99.708, 44.107), kl (0.876, 0.234), total (100.739, 44.497), l2 (0.155)\n",
      "Epoch    52, Epoch time = 0.423 s, Loss (train, valid):  recon (99.737, 44.322), kl (0.874, 0.208), total (100.766, 44.684), l2 (0.155)\n",
      "Epoch    53, Epoch time = 0.415 s, Loss (train, valid):  recon (99.713, 44.194), kl (0.885, 0.212), total (100.752, 44.560), l2 (0.154)\n",
      "Epoch    54, Epoch time = 0.426 s, Loss (train, valid):  recon (99.659, 44.059), kl (0.899, 0.212), total (100.712, 44.423), l2 (0.154)\n",
      "Epoch    55, Epoch time = 0.421 s, Loss (train, valid):  recon (99.595, 44.332), kl (0.933, 0.187), total (100.679, 44.670), l2 (0.151)\n",
      "Epoch    56, Epoch time = 0.408 s, Loss (train, valid):  recon (99.688, 44.183), kl (0.924, 0.214), total (100.760, 44.545), l2 (0.149)\n",
      "Epoch    57, Epoch time = 0.430 s, Loss (train, valid):  recon (99.793, 44.196), kl (0.901, 0.247), total (100.841, 44.591), l2 (0.147)\n",
      "Epoch    58, Epoch time = 0.423 s, Loss (train, valid):  recon (99.665, 44.211), kl (0.895, 0.197), total (100.709, 44.556), l2 (0.149)\n",
      "Epoch    59, Epoch time = 0.451 s, Loss (train, valid):  recon (99.637, 44.147), kl (0.912, 0.277), total (100.694, 44.571), l2 (0.145)\n",
      "Epoch    60, Epoch time = 0.422 s, Loss (train, valid):  recon (99.743, 44.170), kl (0.914, 0.275), total (100.805, 44.591), l2 (0.148)\n",
      "Epoch    61, Epoch time = 0.429 s, Loss (train, valid):  recon (99.650, 44.338), kl (0.938, 0.188), total (100.736, 44.669), l2 (0.147)\n",
      "Epoch 00062: reducing learning rate of group 0 to 9.0250e-03.\n",
      "Epoch    62, Epoch time = 0.423 s, Loss (train, valid):  recon (99.682, 44.340), kl (0.902, 0.198), total (100.728, 44.685), l2 (0.144)\n",
      "Epoch    63, Epoch time = 0.420 s, Loss (train, valid):  recon (99.602, 44.447), kl (0.892, 0.217), total (100.642, 44.809), l2 (0.148)\n",
      "Epoch    64, Epoch time = 0.423 s, Loss (train, valid):  recon (99.647, 44.360), kl (0.918, 0.248), total (100.710, 44.753), l2 (0.146)\n",
      "Epoch    65, Epoch time = 0.425 s, Loss (train, valid):  recon (99.599, 44.089), kl (0.912, 0.193), total (100.655, 44.427), l2 (0.145)\n",
      "Epoch    66, Epoch time = 0.426 s, Loss (train, valid):  recon (99.594, 44.065), kl (0.891, 0.228), total (100.629, 44.438), l2 (0.145)\n",
      "Epoch    67, Epoch time = 0.437 s, Loss (train, valid):  recon (99.607, 44.166), kl (0.920, 0.246), total (100.672, 44.556), l2 (0.146)\n",
      "Epoch    68, Epoch time = 0.415 s, Loss (train, valid):  recon (99.592, 44.123), kl (0.916, 0.203), total (100.654, 44.474), l2 (0.147)\n",
      "Epoch    69, Epoch time = 0.425 s, Loss (train, valid):  recon (99.607, 44.205), kl (0.905, 0.260), total (100.658, 44.610), l2 (0.145)\n",
      "Epoch    70, Epoch time = 0.424 s, Loss (train, valid):  recon (99.621, 43.885), kl (0.904, 0.242), total (100.671, 44.272), l2 (0.145)\n",
      "Epoch    71, Epoch time = 0.419 s, Loss (train, valid):  recon (99.573, 43.791), kl (0.918, 0.205), total (100.637, 44.143), l2 (0.146)\n",
      "Epoch    72, Epoch time = 0.448 s, Loss (train, valid):  recon (99.658, 44.123), kl (0.902, 0.232), total (100.703, 44.498), l2 (0.143)\n",
      "Epoch 00073: reducing learning rate of group 0 to 8.5737e-03.\n",
      "Epoch    73, Epoch time = 0.424 s, Loss (train, valid):  recon (99.597, 44.407), kl (0.904, 0.194), total (100.644, 44.744), l2 (0.142)\n",
      "Epoch    74, Epoch time = 0.425 s, Loss (train, valid):  recon (99.512, 44.233), kl (0.927, 0.230), total (100.583, 44.606), l2 (0.143)\n",
      "Epoch    75, Epoch time = 0.421 s, Loss (train, valid):  recon (99.463, 43.877), kl (0.934, 0.236), total (100.540, 44.257), l2 (0.144)\n",
      "Epoch    76, Epoch time = 0.420 s, Loss (train, valid):  recon (99.626, 44.167), kl (0.880, 0.193), total (100.650, 44.504), l2 (0.144)\n",
      "Epoch    77, Epoch time = 0.431 s, Loss (train, valid):  recon (99.553, 44.355), kl (0.941, 0.153), total (100.637, 44.652), l2 (0.143)\n",
      "Epoch    78, Epoch time = 0.430 s, Loss (train, valid):  recon (99.464, 43.960), kl (0.910, 0.231), total (100.520, 44.337), l2 (0.145)\n",
      "Epoch    79, Epoch time = 0.419 s, Loss (train, valid):  recon (99.461, 43.961), kl (0.940, 0.223), total (100.544, 44.327), l2 (0.143)\n",
      "Epoch    80, Epoch time = 0.468 s, Loss (train, valid):  recon (99.476, 44.074), kl (0.936, 0.234), total (100.556, 44.453), l2 (0.143)\n",
      "Epoch    81, Epoch time = 0.424 s, Loss (train, valid):  recon (99.481, 44.124), kl (0.894, 0.238), total (100.519, 44.504), l2 (0.144)\n",
      "Epoch    82, Epoch time = 0.519 s, Loss (train, valid):  recon (99.460, 44.098), kl (0.949, 0.221), total (100.551, 44.464), l2 (0.143)\n",
      "Epoch    83, Epoch time = 0.430 s, Loss (train, valid):  recon (99.493, 44.061), kl (0.945, 0.220), total (100.581, 44.423), l2 (0.143)\n",
      "Epoch    84, Epoch time = 0.459 s, Loss (train, valid):  recon (99.470, 44.147), kl (0.911, 0.209), total (100.525, 44.498), l2 (0.144)\n",
      "Epoch    85, Epoch time = 0.417 s, Loss (train, valid):  recon (99.513, 44.051), kl (0.917, 0.251), total (100.572, 44.444), l2 (0.142)\n",
      "Epoch    86, Epoch time = 0.432 s, Loss (train, valid):  recon (99.367, 44.422), kl (0.944, 0.210), total (100.453, 44.776), l2 (0.143)\n",
      "Epoch    87, Epoch time = 0.436 s, Loss (train, valid):  recon (99.447, 44.260), kl (0.968, 0.255), total (100.558, 44.658), l2 (0.143)\n",
      "Epoch    88, Epoch time = 0.442 s, Loss (train, valid):  recon (99.556, 44.410), kl (0.943, 0.193), total (100.645, 44.750), l2 (0.146)\n",
      "Epoch    89, Epoch time = 0.430 s, Loss (train, valid):  recon (99.441, 44.278), kl (0.934, 0.201), total (100.520, 44.624), l2 (0.144)\n",
      "Epoch    90, Epoch time = 0.427 s, Loss (train, valid):  recon (99.408, 44.129), kl (0.931, 0.225), total (100.486, 44.504), l2 (0.147)\n",
      "Epoch    91, Epoch time = 0.415 s, Loss (train, valid):  recon (99.497, 44.313), kl (0.909, 0.202), total (100.553, 44.660), l2 (0.147)\n",
      "Epoch    92, Epoch time = 0.429 s, Loss (train, valid):  recon (99.420, 43.954), kl (0.942, 0.233), total (100.508, 44.335), l2 (0.146)\n",
      "Epoch 00093: reducing learning rate of group 0 to 8.1451e-03.\n",
      "Epoch    93, Epoch time = 0.430 s, Loss (train, valid):  recon (99.378, 44.531), kl (0.947, 0.206), total (100.472, 44.884), l2 (0.148)\n",
      "Epoch    94, Epoch time = 0.421 s, Loss (train, valid):  recon (99.422, 44.263), kl (0.975, 0.270), total (100.541, 44.679), l2 (0.145)\n",
      "Epoch    95, Epoch time = 0.423 s, Loss (train, valid):  recon (99.372, 44.185), kl (0.912, 0.205), total (100.431, 44.538), l2 (0.147)\n",
      "Epoch    96, Epoch time = 0.415 s, Loss (train, valid):  recon (99.337, 44.381), kl (0.960, 0.257), total (100.444, 44.783), l2 (0.147)\n",
      "Epoch    97, Epoch time = 0.417 s, Loss (train, valid):  recon (99.328, 44.455), kl (0.928, 0.182), total (100.401, 44.780), l2 (0.145)\n",
      "Epoch    98, Epoch time = 0.447 s, Loss (train, valid):  recon (99.339, 44.082), kl (0.942, 0.250), total (100.423, 44.477), l2 (0.142)\n",
      "Epoch    99, Epoch time = 0.418 s, Loss (train, valid):  recon (99.317, 44.207), kl (0.972, 0.226), total (100.433, 44.577), l2 (0.144)\n",
      "Epoch   100, Epoch time = 0.416 s, Loss (train, valid):  recon (99.367, 43.852), kl (0.925, 0.305), total (100.436, 44.302), l2 (0.144)\n",
      "Epoch     1, Epoch time = 0.439 s, Loss (train, valid):  recon (120.002, 43.957), kl (3.025, 0.337), total (126.538, 48.258), l2 (3.511)\n",
      "Epoch     2, Epoch time = 0.417 s, Loss (train, valid):  recon (103.132, 44.209), kl (0.447, 0.539), total (107.170, 47.938), l2 (3.592)\n",
      "Epoch     3, Epoch time = 0.435 s, Loss (train, valid):  recon (102.251, 43.709), kl (0.647, 0.463), total (105.750, 46.687), l2 (2.852)\n",
      "Epoch     4, Epoch time = 0.441 s, Loss (train, valid):  recon (101.540, 43.524), kl (0.865, 0.598), total (104.650, 46.101), l2 (2.245)\n",
      "Epoch     5, Epoch time = 0.417 s, Loss (train, valid):  recon (100.803, 43.602), kl (1.180, 0.573), total (103.753, 45.739), l2 (1.770)\n",
      "Epoch     6, Epoch time = 0.424 s, Loss (train, valid):  recon (100.505, 43.727), kl (1.192, 0.481), total (103.097, 45.447), l2 (1.400)\n",
      "Epoch     7, Epoch time = 0.429 s, Loss (train, valid):  recon (100.298, 43.511), kl (1.286, 0.484), total (102.699, 44.984), l2 (1.115)\n",
      "Epoch     8, Epoch time = 0.429 s, Loss (train, valid):  recon (100.141, 43.779), kl (1.368, 0.580), total (102.404, 45.157), l2 (0.894)\n",
      "Epoch     9, Epoch time = 0.445 s, Loss (train, valid):  recon (100.077, 43.902), kl (1.336, 0.600), total (102.138, 45.153), l2 (0.725)\n",
      "Epoch    10, Epoch time = 0.419 s, Loss (train, valid):  recon (99.882, 43.403), kl (1.401, 0.680), total (101.878, 44.625), l2 (0.595)\n",
      "Epoch    11, Epoch time = 0.418 s, Loss (train, valid):  recon (99.851, 43.594), kl (1.394, 0.471), total (101.743, 44.521), l2 (0.498)\n",
      "Epoch    12, Epoch time = 0.424 s, Loss (train, valid):  recon (99.679, 43.682), kl (1.400, 0.437), total (101.500, 44.509), l2 (0.422)\n",
      "Epoch    13, Epoch time = 0.422 s, Loss (train, valid):  recon (99.552, 43.575), kl (1.390, 0.577), total (101.305, 44.488), l2 (0.363)\n",
      "Epoch    14, Epoch time = 0.431 s, Loss (train, valid):  recon (99.521, 43.923), kl (1.476, 0.556), total (101.311, 44.772), l2 (0.314)\n",
      "Epoch    15, Epoch time = 0.430 s, Loss (train, valid):  recon (99.611, 43.804), kl (1.432, 0.467), total (101.321, 44.531), l2 (0.278)\n",
      "Epoch    16, Epoch time = 0.428 s, Loss (train, valid):  recon (99.452, 43.829), kl (1.392, 0.456), total (101.095, 44.522), l2 (0.250)\n",
      "Epoch    17, Epoch time = 0.421 s, Loss (train, valid):  recon (99.530, 43.686), kl (1.425, 0.600), total (101.181, 44.503), l2 (0.227)\n",
      "Epoch    18, Epoch time = 0.428 s, Loss (train, valid):  recon (99.324, 44.132), kl (1.376, 0.461), total (100.910, 44.794), l2 (0.211)\n",
      "Epoch    19, Epoch time = 0.431 s, Loss (train, valid):  recon (99.315, 43.792), kl (1.442, 0.513), total (100.952, 44.493), l2 (0.194)\n",
      "Epoch    20, Epoch time = 0.422 s, Loss (train, valid):  recon (99.223, 44.145), kl (1.446, 0.473), total (100.851, 44.795), l2 (0.182)\n",
      "Epoch    21, Epoch time = 0.440 s, Loss (train, valid):  recon (99.260, 43.828), kl (1.421, 0.544), total (100.853, 44.543), l2 (0.172)\n",
      "Epoch    22, Epoch time = 0.428 s, Loss (train, valid):  recon (99.163, 44.282), kl (1.429, 0.492), total (100.761, 44.934), l2 (0.168)\n"
     ]
    }
   ],
   "source": [
    "latent_sizes = np.arange(1, 20)\n",
    "max_epocs = 500\n",
    "\n",
    "for latent_size in latent_sizes:\n",
    "    model = LFADS_SingleSession_Net(\n",
    "        input_size           = 58,\n",
    "        factor_size          = hyperparams['model']['factor_size'],\n",
    "        g_encoder_size       = hyperparams['model']['g_encoder_size'],\n",
    "        c_encoder_size       = hyperparams['model']['c_encoder_size'],\n",
    "        g_latent_size        = int(latent_size),\n",
    "        u_latent_size        = hyperparams['model']['u_latent_size'],\n",
    "        controller_size      = hyperparams['model']['controller_size'],\n",
    "        generator_size       = hyperparams['model']['generator_size'],\n",
    "        prior                = hyperparams['model']['prior'],\n",
    "        clip_val             = hyperparams['model']['clip_val'],\n",
    "        dropout              = hyperparams['model']['dropout'],\n",
    "        do_normalize_factors = hyperparams['model']['normalize_factors'],\n",
    "        max_norm             = hyperparams['model']['max_norm'],\n",
    "        device               = device\n",
    "    ).to(device)\n",
    "    optimizer = opt.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                        lr=hyperparams['optimizer']['lr_init'],\n",
    "                        betas=hyperparams['optimizer']['betas'],\n",
    "                        eps=hyperparams['optimizer']['eps'])\n",
    "\n",
    "    scheduler = LFADS_Scheduler(optimizer      = optimizer,\n",
    "                                mode           = 'min',\n",
    "                                factor         = hyperparams['scheduler']['scheduler_factor'],\n",
    "                                patience       = hyperparams['scheduler']['scheduler_patience'],\n",
    "                                verbose        = True,\n",
    "                                threshold      = 1e-4,\n",
    "                                threshold_mode = 'abs',\n",
    "                                cooldown       = hyperparams['scheduler']['scheduler_cooldown'],\n",
    "                                min_lr         = hyperparams['scheduler']['lr_min'])\n",
    "    run_manager = RunManager(model = model,\n",
    "                         objective  = objective,\n",
    "                         optimizer  = optimizer,\n",
    "                         scheduler  = scheduler,\n",
    "                         train_dl   = train_dl,\n",
    "                         valid_dl   = valid_dl,\n",
    "                         transforms = None,  # transforms,\n",
    "                         writer     = None,\n",
    "                         plotter    = None,\n",
    "                         max_epochs = 100,\n",
    "                         save_loc   = f\"data/lfadscrosslatent{latent_size}\",\n",
    "                         do_health_check = False)\n",
    "    run_manager.run()\n",
    "    with open(f\"data/latent_{latent_size}_loss_data.pickle\", \"wb\") as f:\n",
    "        pickle.dump(run_manager.loss_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
