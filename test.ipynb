{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from lfads import LFADS_Net, LFADS_SingleSession_Net\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "from scheduler import LFADS_Scheduler\n",
    "from plotter import Plotter\n",
    "from trainer import RunManager\n",
    "\n",
    "import yaml\n",
    "\n",
    "from synthetic_data import LorenzSystem, EmbeddedLowDNetwork\n",
    "from objective import SVLAE_Loss, LFADS_Loss, LogLikelihoodPoisson, LogLikelihoodPoissonSimplePlusL1, LogLikelihoodPoissonSimple, LogLikelihoodGaussian\n",
    "\n",
    "from utils import load_parameters\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just using provided defaults here, \n",
    "dt = 0.01\n",
    "num_steps = 1000\n",
    "num_inits = 100\n",
    "lorenz = LorenzSystem(num_inits=num_inits,\n",
    "                      dt= dt)\n",
    "net = EmbeddedLowDNetwork(low_d_system = lorenz, net_size=64, base_rate=1.0, dt= dt)\n",
    "# no burn steps, no inputs, simulate for 10s?\n",
    "# returns time_steps x inits x dimensions\n",
    "rates = net.integrate(burn_steps=0, num_steps=num_steps, inputs=None)\n",
    "\n",
    "latents = net.low_d_system.result\n",
    "\n",
    "# rates x dt, which is 0.01\n",
    "spikes = np.random.poisson(rates * dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100, 64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'; print(device)\n",
    "device = 'cpu'\n",
    "train_idxs, valid_idxs = train_test_split(np.arange(num_inits), train_size=0.8)\n",
    "train_rates = rates[:, train_idxs, :]\n",
    "train_latents = latents[:, train_idxs, :]\n",
    "train_spikes = spikes[:, train_idxs, :]\n",
    "\n",
    "valid_rates = rates[:, valid_idxs, :]\n",
    "valid_latents = latents[:, valid_idxs, :]\n",
    "valid_spikes = spikes[:, valid_idxs, :]\n",
    "\n",
    "train_data  = torch.Tensor(train_spikes).to(device)\n",
    "valid_data  = torch.Tensor(valid_spikes).to(device)\n",
    "print(train_data.device)\n",
    "train_ds    = torch.utils.data.TensorDataset(train_data)\n",
    "valid_ds    = torch.utils.data.TensorDataset(valid_data)\n",
    "train_dl    = torch.utils.data.DataLoader(train_ds, batch_size = 25, shuffle=True)\n",
    "valid_dl    = torch.utils.data.DataLoader(valid_ds, batch_size = valid_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_path = '../hierarchical_lfads/hyperparameters/lorenz/lfads.yaml'\n",
    "hyperparams = load_parameters(hyperparameter_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LFADS_SingleSession_Net(\n",
    "    input_size           = 64,\n",
    "    factor_size          = hyperparams['model']['factor_size'],\n",
    "    g_encoder_size       = hyperparams['model']['g_encoder_size'],\n",
    "    c_encoder_size       = hyperparams['model']['c_encoder_size'],\n",
    "    g_latent_size        = hyperparams['model']['g_latent_size'],\n",
    "    u_latent_size        = hyperparams['model']['u_latent_size'],\n",
    "    controller_size      = hyperparams['model']['controller_size'],\n",
    "    generator_size       = hyperparams['model']['generator_size'],\n",
    "    prior                = hyperparams['model']['prior'],\n",
    "    clip_val             = hyperparams['model']['clip_val'],\n",
    "    dropout              = hyperparams['model']['dropout'],\n",
    "    do_normalize_factors = hyperparams['model']['normalize_factors'],\n",
    "    max_norm             = hyperparams['model']['max_norm'],\n",
    "    device               = device\n",
    ").to(device)\n",
    "\n",
    "# model = LFADS_Net(\n",
    "#     input_size           = 64,\n",
    "#     factor_size          = 3,\n",
    "#     g_encoder_size       = hyperparams['model']['g_encoder_size'],\n",
    "#     c_encoder_size       = hyperparams['model']['c_encoder_size'],\n",
    "#     g_latent_size        = 3,\n",
    "#     u_latent_size        = hyperparams['model']['u_latent_size'],\n",
    "#     controller_size      = hyperparams['model']['controller_size'],\n",
    "#     generator_size       = hyperparams['model']['generator_size'],\n",
    "#     prior                = hyperparams['model']['prior'],\n",
    "#     clip_val             = hyperparams['model']['clip_val'],\n",
    "#     dropout              = hyperparams['model']['dropout'],\n",
    "#     do_normalize_factors = hyperparams['model']['normalize_factors'],\n",
    "#     max_norm             = hyperparams['model']['max_norm'],\n",
    "#     device               = 'cuda:0'\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglikelihood = LogLikelihoodPoisson(dt=dt, device=device)\n",
    "objective = LFADS_Loss(\n",
    "    loglikelihood=loglikelihood,\n",
    "    loss_weight_dict={\n",
    "        \"kl\": hyperparams['objective'][\"kl\"],\n",
    "        \"l2\": hyperparams[\"objective\"][\"l2\"]},\n",
    "    l2_con_scale=hyperparams[\"objective\"][\"l2_con_scale\"],\n",
    "    l2_gen_scale=hyperparams[\"objective\"][\"l2_gen_scale\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = opt.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                     lr=hyperparams['optimizer']['lr_init'],\n",
    "                     betas=hyperparams['optimizer']['betas'],\n",
    "                     eps=hyperparams['optimizer']['eps'])\n",
    "\n",
    "scheduler = LFADS_Scheduler(optimizer      = optimizer,\n",
    "                            mode           = 'min',\n",
    "                            factor         = hyperparams['scheduler']['scheduler_factor'],\n",
    "                            patience       = hyperparams['scheduler']['scheduler_patience'],\n",
    "                            verbose        = True,\n",
    "                            threshold      = 1e-4,\n",
    "                            threshold_mode = 'abs',\n",
    "                            cooldown       = hyperparams['scheduler']['scheduler_cooldown'],\n",
    "                            min_lr         = hyperparams['scheduler']['lr_min'])\n",
    "\n",
    "TIME = torch.arange(0, num_steps*dt, dt)\n",
    "\n",
    "plotter = {\n",
    "    'train' : Plotter(time=TIME, truth={\n",
    "        'rates'   : train_rates,\n",
    "        'spikes'  : train_spikes,\n",
    "        'latent'  : train_latents}),\n",
    "    'valid' : Plotter(time=TIME, truth={\n",
    "        'rates'   : valid_rates,\n",
    "        'spikes'  : valid_spikes,\n",
    "        'latent'  : valid_latents}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20, 64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_spikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms  = trf.Compose([])\n",
    "\n",
    "run_manager = RunManager(model      = model,\n",
    "                         objective  = objective,\n",
    "                         optimizer  = optimizer,\n",
    "                         scheduler  = scheduler,\n",
    "                         train_dl   = train_dl,\n",
    "                         valid_dl   = valid_dl,\n",
    "                         transforms = None,  # transforms,\n",
    "                         writer     = None,\n",
    "                         plotter    = plotter,\n",
    "                         max_epochs = 5,\n",
    "                         save_loc   = \"data/lorenz\",\n",
    "                         do_health_check = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     1, Epoch time = 6.066 s, Loss (train, valid):  recon (348.897, 85.977), kl (1.694, 1.553), total (350.636, 87.624), l2 (0.045)\n",
      "Epoch     2, Epoch time = 6.057 s, Loss (train, valid):  recon (345.893, 85.606), kl (0.457, 0.068), total (346.490, 85.864), l2 (0.141)\n",
      "Epoch     3, Epoch time = 6.125 s, Loss (train, valid):  recon (345.201, 85.581), kl (0.312, 0.094), total (345.747, 85.954), l2 (0.233)\n",
      "Epoch     4, Epoch time = 6.136 s, Loss (train, valid):  recon (345.092, 85.573), kl (0.342, 0.120), total (345.755, 86.055), l2 (0.320)\n",
      "Epoch     5, Epoch time = 6.238 s, Loss (train, valid):  recon (345.073, 85.559), kl (0.342, 0.138), total (345.815, 86.134), l2 (0.400)\n"
     ]
    }
   ],
   "source": [
    "run_manager.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
